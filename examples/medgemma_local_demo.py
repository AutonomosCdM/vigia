#!/usr/bin/env python3
"""
Demo MedGemma Local - Demostraci√≥n de MedGemma ejecutando localmente
Ejemplo completo de uso de MedGemma local para an√°lisis m√©dico.

Requisitos:
1. Ejecutar: python scripts/setup_medgemma_local.py --install-deps
2. Ejecutar: python scripts/setup_medgemma_local.py --model 4b --download
3. Ejecutar: python examples/medgemma_local_demo.py
"""

import asyncio
import sys
from pathlib import Path
from typing import Dict, Any

# Agregar path del proyecto
sys.path.append(str(Path(__file__).parent.parent))

from vigia_detect.ai.medgemma_local_client import (
    MedGemmaLocalClient,
    MedGemmaConfig,
    MedGemmaModel,
    MedGemmaRequest,
    MedicalContext,
    InferenceMode,
    MedGemmaLocalFactory
)


class MedGemmaLocalDemo:
    """Demo de MedGemma Local."""
    
    def __init__(self):
        self.client = None
    
    async def initialize(self):
        """Inicializar cliente MedGemma."""
        print("ü§ñ Inicializando MedGemma Local...")
        
        # Verificar requisitos
        requirements = MedGemmaLocalFactory.check_requirements()
        
        if not requirements['torch_available']:
            print("‚ùå PyTorch no disponible. Instalar con: pip install torch")
            return False
        
        if not requirements['transformers_available']:
            print("‚ùå Transformers no disponible. Instalar con: pip install transformers")
            return False
        
        print("‚úÖ Requisitos verificados")
        
        # Configurar cliente
        config = MedGemmaConfig(
            model_name=MedGemmaModel.MEDGEMMA_4B_IT,
            device="auto",  # auto-detectar mejor dispositivo
            quantization=True,  # Usar quantizaci√≥n para menor memoria
            max_tokens=200,
            temperature=0.7,
            local_files_only=True  # Solo usar archivos descargados
        )
        
        try:
            self.client = await MedGemmaLocalFactory.create_client(config)
            print("‚úÖ MedGemma Local inicializado correctamente")
            
            # Mostrar estad√≠sticas
            stats = await self.client.get_stats()
            print(f"   Modelo: {stats['model_name']}")
            print(f"   Dispositivo: {stats['device']}")
            
            if 'memory_usage' in stats:
                memory = stats['memory_usage']
                if 'cuda_allocated' in memory:
                    print(f"   Memoria GPU: {memory['cuda_allocated']:.1f} GB")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Error inicializando MedGemma: {e}")
            print("üí° Aseg√∫rate de haber descargado el modelo con:")
            print("   python scripts/setup_medgemma_local.py --model 4b --download")
            return False
    
    async def demo_basic_consultation(self):
        """Demo de consulta m√©dica b√°sica."""
        print("\n" + "="*60)
        print("ü©∫ DEMO 1: Consulta M√©dica B√°sica")
        print("="*60)
        
        request = MedGemmaRequest(
            text_prompt="¬øCu√°les son los signos de una lesi√≥n por presi√≥n grado 2?",
            inference_mode=InferenceMode.TEXT_ONLY,
            max_tokens=150
        )
        
        print(f"Consulta: {request.text_prompt}")
        print("\nü§ñ Respuesta de MedGemma:")
        print("-" * 40)
        
        response = await self.client.generate_medical_response(request)
        
        if response.success:
            print(response.generated_text)
            print(f"\nüìä Confianza: {response.confidence_score:.2f}")
            print(f"‚è±Ô∏è Tiempo: {response.processing_time:.2f}s")
            
            if response.warnings:
                print(f"‚ö†Ô∏è Advertencias: {', '.join(response.warnings)}")
        else:
            print(f"‚ùå Error: {response.error_message}")
    
    async def demo_with_context(self):
        """Demo con contexto m√©dico."""
        print("\n" + "="*60)
        print("üè• DEMO 2: Consulta con Contexto M√©dico")
        print("="*60)
        
        # Crear contexto m√©dico
        context = MedicalContext(
            patient_age=75,
            patient_gender="femenino",
            medical_history="Diabetes tipo 2, hipertensi√≥n arterial",
            current_medications=["Metformina", "Enalapril"],
            symptoms="Eritema en regi√≥n sacra, no blanqueable",
            urgency_level="urgent"
        )
        
        request = MedGemmaRequest(
            text_prompt="Eval√∫a esta posible lesi√≥n por presi√≥n y recomienda tratamiento",
            medical_context=context,
            inference_mode=InferenceMode.TEXT_ONLY,
            max_tokens=250
        )
        
        print("üë§ Contexto del paciente:")
        print(f"   Edad: {context.patient_age} a√±os")
        print(f"   G√©nero: {context.patient_gender}")
        print(f"   Historia: {context.medical_history}")
        print(f"   Medicamentos: {', '.join(context.current_medications)}")
        print(f"   S√≠ntomas: {context.symptoms}")
        print(f"   Urgencia: {context.urgency_level}")
        
        print(f"\nConsulta: {request.text_prompt}")
        print("\nü§ñ Respuesta de MedGemma:")
        print("-" * 40)
        
        response = await self.client.generate_medical_response(request)
        
        if response.success:
            print(response.generated_text)
            
            print(f"\nüìä An√°lisis m√©dico:")
            analysis = response.medical_analysis
            print(f"   Confianza: {response.confidence_score:.2f}")
            print(f"   Urgencia detectada: {analysis.get('urgency_assessment', 'N/A')}")
            
            if analysis.get('recommendations'):
                print("   Recomendaciones extra√≠das:")
                for rec in analysis['recommendations'][:3]:
                    print(f"     ‚Ä¢ {rec}")
            
            print(f"\n‚è±Ô∏è Tiempo de procesamiento: {response.processing_time:.2f}s")
        else:
            print(f"‚ùå Error: {response.error_message}")
    
    async def demo_cache_performance(self):
        """Demo de performance con cache."""
        print("\n" + "="*60)
        print("‚ö° DEMO 3: Performance y Cache")
        print("="*60)
        
        request = MedGemmaRequest(
            text_prompt="¬øCu√°l es el protocolo de prevenci√≥n de LPP en UCI?",
            inference_mode=InferenceMode.TEXT_ONLY,
            max_tokens=100
        )
        
        print("üîÑ Primera consulta (sin cache):")
        response1 = await self.client.generate_medical_response(request)
        print(f"   Tiempo: {response1.processing_time:.2f}s")
        
        print("\nüîÑ Segunda consulta id√©ntica (con cache):")
        response2 = await self.client.generate_medical_response(request)
        print(f"   Tiempo: {response2.processing_time:.2f}s")
        
        # Mostrar estad√≠sticas
        stats = await self.client.get_stats()
        print(f"\nüìà Estad√≠sticas:")
        print(f"   Consultas procesadas: {stats['requests_processed']}")
        print(f"   Cache hits: {stats['cache_hits']}")
        print(f"   Tasa de cache: {stats['cache_hit_rate']:.2%}")
        print(f"   Tiempo promedio: {stats['average_processing_time']:.2f}s")
        print(f"   Tokens generados: {stats['total_tokens_generated']}")
    
    async def demo_error_handling(self):
        """Demo de manejo de errores."""
        print("\n" + "="*60)
        print("üõ°Ô∏è DEMO 4: Manejo de Errores")
        print("="*60)
        
        # Consulta con tokens excesivos
        request = MedGemmaRequest(
            text_prompt="Explica todo sobre medicina",
            inference_mode=InferenceMode.TEXT_ONLY,
            max_tokens=10000  # Excesivo intencionalmente
        )
        
        print("üö´ Consulta con par√°metros extremos:")
        response = await self.client.generate_medical_response(request)
        
        if response.success:
            print("‚úÖ Manejado correctamente - respuesta generada")
            print(f"   Longitud: {len(response.generated_text)} caracteres")
        else:
            print(f"‚ùå Error controlado: {response.error_message}")
        
        # Consulta ambigua
        ambiguous_request = MedGemmaRequest(
            text_prompt="¬øQu√© hago?",
            inference_mode=InferenceMode.TEXT_ONLY,
            max_tokens=100
        )
        
        print("\n‚ùì Consulta ambigua:")
        print(f"   Consulta: {ambiguous_request.text_prompt}")
        
        response = await self.client.generate_medical_response(ambiguous_request)
        
        if response.success:
            print(f"   Respuesta: {response.generated_text[:100]}...")
            print(f"   Confianza: {response.confidence_score:.2f}")
            
            if response.warnings:
                print(f"   ‚ö†Ô∏è Advertencias: {', '.join(response.warnings)}")
        else:
            print(f"‚ùå Error: {response.error_message}")
    
    async def run_all_demos(self):
        """Ejecutar todas las demostraciones."""
        print("üöÄ MedGemma Local - Demostraci√≥n Completa")
        print("="*70)
        
        # Inicializar
        if not await self.initialize():
            return
        
        try:
            # Ejecutar demos
            await self.demo_basic_consultation()
            await self.demo_with_context()
            await self.demo_cache_performance()
            await self.demo_error_handling()
            
            print("\n" + "="*70)
            print("‚úÖ Demostraci√≥n completada exitosamente")
            
            # Estad√≠sticas finales
            stats = await self.client.get_stats()
            print(f"\nüìä Estad√≠sticas finales:")
            print(f"   Total de consultas: {stats['requests_processed']}")
            print(f"   Tiempo promedio: {stats['average_processing_time']:.2f}s")
            print(f"   Tokens generados: {stats['total_tokens_generated']}")
            print(f"   Errores: {stats['errors']}")
            
        except KeyboardInterrupt:
            print("\n‚èπÔ∏è Demo interrumpida por usuario")
        
        except Exception as e:
            print(f"\n‚ùå Error en demo: {e}")
        
        finally:
            # Limpiar recursos
            if self.client:
                await self.client.cleanup()
                print("üßπ Recursos liberados")


async def main():
    """Funci√≥n principal."""
    demo = MedGemmaLocalDemo()
    await demo.run_all_demos()


if __name__ == "__main__":
    print("ü§ñ MedGemma Local Demo")
    print("Aseg√∫rate de haber instalado dependencias y descargado el modelo:")
    print("1. python scripts/setup_medgemma_local.py --install-deps")
    print("2. python scripts/setup_medgemma_local.py --model 4b --download")
    print("\nPresiona Ctrl+C para interrumpir en cualquier momento\n")
    
    asyncio.run(main())