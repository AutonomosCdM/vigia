# MedGemma Local Setup Guide

Gu√≠a completa para configurar y usar MedGemma localmente en el sistema Vigia.

## üìã Tabla de Contenidos

1. [Ventajas de MedGemma Local](#ventajas)
2. [Requisitos del Sistema](#requisitos)
3. [Instalaci√≥n](#instalaci√≥n)
4. [Configuraci√≥n](#configuraci√≥n)
5. [Uso](#uso)
6. [Troubleshooting](#troubleshooting)

## üéØ Ventajas de MedGemma Local vs API

### ‚úÖ **MedGemma Local (Recomendado)**
- üîí **Privacidad total**: Datos m√©dicos nunca salen del servidor
- üí∞ **Sin costos**: No hay cargos por consulta
- ‚ö° **Latencia predecible**: Sin dependencia de internet
- üéõÔ∏è **Control total**: Par√°metros personalizables
- üîê **Compliance**: Cumple HIPAA out-of-the-box
- üìä **Escalabilidad**: Performance constante

### ‚ùå **API Externa (Problem√°tico)**
- üåê Requiere internet estable
- üí∏ Costo por uso
- üîë Gesti√≥n de API keys
- üö® Datos enviados a Google
- ‚è±Ô∏è Latencia variable
- üìà Escalabilidad limitada por cuotas

## üíª Requisitos del Sistema

### M√≠nimos
- **RAM**: 16GB+ (recomendado 32GB)
- **GPU**: 8GB VRAM (modelo 4B) o 32GB VRAM (modelo 27B)
- **Almacenamiento**: 20GB libres para modelo + cache
- **Python**: 3.8+

### Recomendados
- **GPU**: NVIDIA RTX 3080/4080 o superior
- **CUDA**: 11.8+ o 12.0+
- **RAM**: 32GB+
- **SSD**: Para mejor performance de carga

### Verificar Requisitos
```bash
python scripts/setup_medgemma_local.py --check-only
```

## üöÄ Instalaci√≥n

### 1. Instalar Dependencias

```bash
# Instalar dependencias b√°sicas
python scripts/setup_medgemma_local.py --install-deps

# O manualmente:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers>=4.50.0 accelerate bitsandbytes sentencepiece
```

### 2. Verificar Instalaci√≥n

```bash
python scripts/setup_medgemma_local.py --list
```

### 3. Descargar Modelo

```bash
# Modelo 4B (recomendado para la mayor√≠a)
python scripts/setup_medgemma_local.py --model 4b --download

# Modelo 27B (para servidores potentes)
python scripts/setup_medgemma_local.py --model 27b --download
```

### 4. Probar Instalaci√≥n

```bash
python scripts/setup_medgemma_local.py --model 4b --test
```

## ‚öôÔ∏è Configuraci√≥n

### Selecci√≥n de Modelo

| Modelo | Par√°metros | Memoria GPU | Tipo | Uso Recomendado |
|--------|------------|-------------|------|-----------------|
| `4b-it` | 4B | ~8GB | Multimodal | **Recomendado** - Texto e im√°genes |
| `4b-pt` | 4B | ~8GB | Multimodal | Fine-tuning personalizado |
| `27b-text` | 27B | ~32GB | Solo texto | Servidores de alta gama |

### Configuraci√≥n por Hardware

```python
# GPU potente (RTX 4080+)
config = MedGemmaConfig(
    model_name=MedGemmaModel.MEDGEMMA_4B_IT,
    device="cuda",
    quantization=False,  # Sin quantizaci√≥n
    torch_dtype="bfloat16"
)

# GPU est√°ndar (RTX 3080)
config = MedGemmaConfig(
    model_name=MedGemmaModel.MEDGEMMA_4B_IT,
    device="cuda",
    quantization=True,  # Con quantizaci√≥n
    torch_dtype="bfloat16"
)

# CPU (desarrollo/testing)
config = MedGemmaConfig(
    model_name=MedGemmaModel.MEDGEMMA_4B_IT,
    device="cpu",
    quantization=False,
    torch_dtype="float32"
)
```

## üîß Uso

### 1. Uso B√°sico

```python
import asyncio
from vigia_detect.ai.medgemma_local_client import (
    MedGemmaLocalClient, MedGemmaConfig, MedGemmaModel,
    MedGemmaRequest, InferenceMode
)

async def basic_usage():
    # Configurar cliente
    config = MedGemmaConfig(
        model_name=MedGemmaModel.MEDGEMMA_4B_IT,
        device="auto"
    )
    
    client = MedGemmaLocalClient(config)
    await client.initialize()
    
    # Crear consulta
    request = MedGemmaRequest(
        text_prompt="¬øCu√°les son los signos de una LPP grado 2?",
        inference_mode=InferenceMode.TEXT_ONLY,
        max_tokens=200
    )
    
    # Generar respuesta
    response = await client.generate_medical_response(request)
    
    if response.success:
        print(f"Respuesta: {response.generated_text}")
        print(f"Confianza: {response.confidence_score}")
    
    await client.cleanup()

# Ejecutar
asyncio.run(basic_usage())
```

### 2. Con Contexto M√©dico

```python
from vigia_detect.ai.medgemma_local_client import MedicalContext

# Crear contexto del paciente
context = MedicalContext(
    patient_age=75,
    patient_gender="femenino",
    medical_history="Diabetes tipo 2, hipertensi√≥n",
    current_medications=["Metformina", "Enalapril"],
    symptoms="Eritema en regi√≥n sacra",
    urgency_level="urgent"
)

request = MedGemmaRequest(
    text_prompt="Eval√∫a esta posible lesi√≥n por presi√≥n",
    medical_context=context,
    inference_mode=InferenceMode.TEXT_ONLY
)
```

### 3. An√°lisis Multimodal (con imagen)

```python
request = MedGemmaRequest(
    text_prompt="Analiza esta imagen m√©dica y describe los hallazgos",
    image_path="/path/to/medical_image.jpg",
    inference_mode=InferenceMode.IMAGE_TEXT,
    max_tokens=300
)
```

### 4. Demo Completo

```bash
python examples/medgemma_local_demo.py
```

## üìä Monitoreo y Estad√≠sticas

```python
# Obtener estad√≠sticas
stats = await client.get_stats()
print(f"Consultas procesadas: {stats['requests_processed']}")
print(f"Tiempo promedio: {stats['average_processing_time']:.2f}s")
print(f"Cache hit rate: {stats['cache_hit_rate']:.2%}")
print(f"Memoria GPU: {stats['memory_usage']['cuda_allocated']:.1f}GB")
```

## üõ†Ô∏è Troubleshooting

### Error: "CUDA out of memory"

```bash
# Soluci√≥n 1: Usar quantizaci√≥n
config.quantization = True

# Soluci√≥n 2: Reducir max_tokens
request.max_tokens = 100

# Soluci√≥n 3: Limpiar cache
torch.cuda.empty_cache()
```

### Error: "Model not found"

```bash
# Re-descargar modelo
python scripts/setup_medgemma_local.py --model 4b --download

# Verificar ubicaci√≥n
ls ~/.cache/huggingface/transformers/
```

### Error: "Transformers version"

```bash
# Actualizar transformers
pip install transformers>=4.50.0 --upgrade
```

### Performance Lenta

```bash
# Verificar device
print(f"Device: {client.device}")

# Usar GPU si disponible
config.device = "cuda"

# Habilitar quantizaci√≥n
config.quantization = True
```

### Problemas de Memoria

```bash
# Monitorear memoria
nvidia-smi

# Limpiar cache entre consultas
await client.cleanup()
```

## üîß Configuraci√≥n Avanzada

### Variables de Entorno

```bash
# Cache personalizado
export TRANSFORMERS_CACHE="/custom/path/cache"

# Device espec√≠fico
export CUDA_VISIBLE_DEVICES="0"

# Logging
export TRANSFORMERS_VERBOSITY="error"
```

### Optimizaci√≥n de Performance

```python
# Para batch processing
config = MedGemmaConfig(
    model_name=MedGemmaModel.MEDGEMMA_4B_IT,
    device="cuda",
    quantization=True,
    torch_dtype="bfloat16",
    max_tokens=150,  # Limitar para mayor velocidad
    temperature=0.7
)
```

### Fine-tuning (Avanzado)

```python
# Para fine-tuning personalizado
config = MedGemmaConfig(
    model_name=MedGemmaModel.MEDGEMMA_4B_PT,  # Pre-trained
    device="cuda",
    quantization=False,  # Sin quantizaci√≥n para training
    torch_dtype="float32"
)
```

## üìà Benchmarks

### Performance T√≠pica (RTX 4080)

| Modelo | Tokens/seg | Memoria | Latencia |
|--------|------------|---------|----------|
| 4B (sin quant) | ~50 | 8GB | 2-4s |
| 4B (con quant) | ~40 | 6GB | 2-5s |
| 27B (sin quant) | ~20 | 32GB | 5-10s |

### Comparaci√≥n vs API

| M√©trica | Local | API |
|---------|-------|-----|
| Primera consulta | 3-5s | 2-8s |
| Consultas cached | 0.1s | 2-8s |
| Costo por 1000 consultas | $0 | $10-50 |
| Privacidad | 100% | Depende |

## üîí Consideraciones de Seguridad

### Datos M√©dicos
- ‚úÖ Todos los datos permanecen locales
- ‚úÖ No hay transmisi√≥n a servicios externos
- ‚úÖ Compatible con HIPAA/GDPR out-of-the-box
- ‚úÖ Control total sobre logs y audit trails

### Modelo
- ‚ö†Ô∏è MedGemma no es "clinical-grade" por defecto
- ‚ö†Ô∏è Requiere validaci√≥n m√©dica antes de producci√≥n
- ‚ö†Ô∏è Debe complementarse con supervisi√≥n humana
- ‚úÖ Dise√±ado espec√≠ficamente para casos m√©dicos

## üìö Recursos Adicionales

- [MedGemma Official Documentation](https://developers.google.com/health-ai-developer-foundations/medgemma)
- [Hugging Face Model Hub](https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4)
- [Fine-tuning Guide](https://github.com/google-deepmind/gemma/blob/main/examples/medgemma_fine_tuning.ipynb)
- [Vigia Medical Knowledge System](./medical_knowledge_system.md)

## üÜò Soporte

Para problemas espec√≠ficos:

1. **Verificar logs**: `tail -f logs/medgemma_local_client.log`
2. **Diagnosticar hardware**: `python scripts/setup_medgemma_local.py --check-only`
3. **Limpiar instalaci√≥n**: Eliminar `~/.cache/huggingface/`
4. **Reportar issues**: [GitHub Issues](../../issues)

---

**Importante**: MedGemma Local es una herramienta poderosa pero debe usarse con supervisi√≥n m√©dica apropiada en entornos de producci√≥n cl√≠nica.