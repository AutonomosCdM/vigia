#!/usr/bin/env python3
"""
Vigia Medical AI Demo - Comprehensive MedGemma Integration
Demonstrates complete medical AI workflow with local and API-based processing.
"""

import asyncio
import json
import os
import sys
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
from pprint import pprint

# Add project path
sys.path.append(str(Path(__file__).parent.parent))

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    from vigia_detect.ai.medgemma_client import MedGemmaClient, MedicalContext, MedicalAnalysisType
    from vigia_detect.ai.medical_prompts import MedicalPromptBuilder, PromptTemplate, MedicalPromptContext
    from vigia_detect.core.triage_engine import MedicalTriageEngine
    from vigia_detect.core.input_packager import InputPackager, InputType
    from config.settings import get_settings
except ImportError as e:
    logger.error(f"Import error: {e}")
    print("‚ö†Ô∏è  Some components may not be available. Install dependencies or check imports.")


class MedicalAIDemo:
    """Comprehensive medical AI demonstration with MedGemma integration."""
    
    def __init__(self):
        """Initialize medical AI demo."""
        self.medgemma_client = None
        self.triage_engine = None
        self.settings = None
        
    async def initialize(self):
        """Initialize medical AI components."""
        try:
            # Initialize settings
            self.settings = get_settings()
            
            # Initialize MedGemma client
            self.medgemma_client = MedGemmaClient()
            
            # Initialize triage engine
            self.triage_engine = MedicalTriageEngine()
            
            logger.info("‚úÖ Medical AI components initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize components: {e}")
            return False
    
    async def demo_basic_medical_analysis(self):
        """Demonstrate basic medical analysis with MedGemma."""
        print("\n" + "="*60)
        print("üß† DEMO 1: Basic Medical Analysis")
        print("="*60)
        
        # Test cases for different LPP stages
        test_cases = [
            {
                "name": "Stage 1 LPP - Early Detection",
                "observations": "Paciente presenta eritema no blanqueable en regi√≥n sacra de 2cm de di√°metro, piel intacta sin p√©rdida de tejido",
                "context": MedicalContext(
                    patient_age=78,
                    diabetes=True,
                    mobility_level="bed_bound",
                    braden_score=12
                )
            },
            {
                "name": "Stage 2 LPP - Partial Thickness",
                "observations": "√ölcera superficial con p√©rdida parcial del grosor de la piel en tal√≥n derecho, lecho de la herida rosado-rojizo",
                "context": MedicalContext(
                    patient_age=65,
                    diabetes=False,
                    mobility_level="chair_bound",
                    braden_score=15
                )
            },
            {
                "name": "Stage 3 LPP - Full Thickness",
                "observations": "P√©rdida completa del grosor de la piel en regi√≥n sacra con tejido subcut√°neo visible, sin exposici√≥n √≥sea",
                "context": MedicalContext(
                    patient_age=82,
                    diabetes=True,
                    mobility_level="bed_bound",
                    braden_score=8
                )
            }
        ]
        
        for i, case in enumerate(test_cases, 1):
            print(f"\nüî¨ Test Case {i}: {case['name']}")
            print("-" * 40)
            
            try:
                # Analyze with MedGemma
                result = await self.medgemma_client.analyze_lpp_findings(
                    clinical_observations=case["observations"],
                    medical_context=case["context"]
                )
                
                if result:
                    print(f"‚úÖ Analysis completed:")
                    print(f"   üéØ Confidence: {result.confidence_score:.1%}")
                    print(f"   üìã Clinical Findings: {result.clinical_findings}")
                    print(f"   üí° Recommendations: {result.recommendations}")
                    if hasattr(result, 'urgency_level'):
                        print(f"   üö® Urgency: {result.urgency_level}")
                else:
                    print("‚ùå No analysis result obtained")
                    
            except Exception as e:
                print(f"‚ùå Analysis failed: {e}")
                
    async def demo_medical_image_analysis(self):
        """Demonstrate medical image analysis workflow."""
        print("\n" + "="*60)
        print("üñºÔ∏è  DEMO 2: Medical Image Analysis Workflow")
        print("="*60)
        
        # Simulate image analysis results
        simulated_detection = {
            "lpp_stage": 2,
            "confidence": 0.87,
            "bounding_box": [150, 200, 300, 350],
            "anatomical_location": "sacrum"
        }
        
        print("üì∏ Simulated Image Detection Results:")
        print(f"   Stage: {simulated_detection['lpp_stage']}")
        print(f"   Confidence: {simulated_detection['confidence']:.1%}")
        print(f"   Location: {simulated_detection['anatomical_location']}")
        
        # Create clinical context from detection
        clinical_context = f"""
        Image analysis detected Stage {simulated_detection['lpp_stage']} pressure injury 
        at {simulated_detection['anatomical_location']} with {simulated_detection['confidence']:.1%} confidence.
        Bounding box coordinates: {simulated_detection['bounding_box']}.
        """
        
        print("\nüß† MedGemma Analysis of Detection:")
        try:
            medical_context = MedicalContext(
                patient_age=70,
                diabetes=True,
                detection_confidence=simulated_detection['confidence']
            )
            
            result = await self.medgemma_client.analyze_lpp_findings(
                clinical_observations=clinical_context,
                medical_context=medical_context
            )
            
            if result:
                print(f"‚úÖ Medical interpretation:")
                print(f"   üìä Clinical Assessment: {result.clinical_findings}")
                print(f"   üíä Treatment Plan: {result.recommendations}")
                print(f"   ‚è∞ Follow-up: {getattr(result, 'follow_up_timeline', 'As needed')}")
            else:
                print("‚ùå Medical interpretation failed")
                
        except Exception as e:
            print(f"‚ùå Medical analysis error: {e}")
    
    async def demo_triage_workflow(self):
        """Demonstrate medical triage workflow."""
        print("\n" + "="*60)
        print("üè• DEMO 3: Medical Triage Workflow")
        print("="*60)
        
        # Simulate patient data
        patient_data = {
            "patient_id": "CD-2025-001",
            "age": 75,
            "medical_history": {
                "diabetes": True,
                "hypertension": True,
                "mobility": "limited"
            },
            "current_findings": "Eritema persistente en coxis con induraci√≥n",
            "braden_score": 11,
            "risk_level": "high"
        }
        
        print("üìã Patient Data:")
        pprint(patient_data, indent=2)
        
        try:
            # Create input package
            input_package = InputPackager.create_medical_package(
                patient_code=patient_data["patient_id"],
                clinical_data=patient_data,
                input_type=InputType.CLINICAL_ASSESSMENT
            )
            
            print("\n‚öïÔ∏è  Processing triage assessment...")
            
            # Process through triage engine
            if self.triage_engine:
                triage_result = await self.triage_engine.process_medical_input(input_package)
                
                print("‚úÖ Triage Results:")
                print(f"   üéØ Priority Level: {triage_result.get('priority', 'standard')}")
                print(f"   üö® Urgency: {triage_result.get('urgency', 'routine')}")
                print(f"   üìù Assessment: {triage_result.get('clinical_assessment', 'N/A')}")
                print(f"   üí° Next Steps: {triage_result.get('recommended_actions', 'Continue monitoring')}")
            else:
                print("‚ö†Ô∏è  Triage engine not available")
                
        except Exception as e:
            print(f"‚ùå Triage workflow error: {e}")
    
    async def demo_prompt_engineering(self):
        """Demonstrate medical prompt engineering."""
        print("\n" + "="*60)
        print("üìù DEMO 4: Medical Prompt Engineering")
        print("="*60)
        
        try:
            # Create prompt builder
            prompt_builder = MedicalPromptBuilder()
            
            # Example prompt contexts
            contexts = [
                {
                    "name": "LPP Assessment",
                    "template": PromptTemplate.LPP_ASSESSMENT,
                    "context": MedicalPromptContext(
                        clinical_findings="Eritema no blanqueable 3x2cm en tal√≥n",
                        patient_context={"age": 80, "diabetes": True},
                        medical_urgency="moderate"
                    )
                },
                {
                    "name": "Treatment Planning",
                    "template": PromptTemplate.TREATMENT_PLANNING,
                    "context": MedicalPromptContext(
                        clinical_findings="Stage 2 LPP confirmed",
                        patient_context={"mobility": "bed_bound", "nutrition": "poor"},
                        medical_urgency="high"
                    )
                }
            ]
            
            for context in contexts:
                print(f"\nüìã {context['name']} Prompt:")
                print("-" * 30)
                
                prompt = prompt_builder.build_prompt(
                    template=context["template"],
                    context=context["context"]
                )
                
                # Show structured prompt
                print("üîß Generated Prompt Structure:")
                print(f"   Template: {context['template'].value}")
                print(f"   Clinical Focus: {context['context'].clinical_findings}")
                print(f"   Patient Context: {context['context'].patient_context}")
                print(f"   Urgency Level: {context['context'].medical_urgency}")
                
        except Exception as e:
            print(f"‚ùå Prompt engineering demo error: {e}")
    
    async def demo_local_vs_api_comparison(self):
        """Compare local vs API-based medical AI processing."""
        print("\n" + "="*60)
        print("‚öñÔ∏è  DEMO 5: Local vs API Processing Comparison")
        print("="*60)
        
        test_query = "Paciente con √∫lcera de 4cm en regi√≥n sacra con tejido necr√≥tico visible"
        
        print(f"üî¨ Test Query: {test_query}")
        print("\nüìä Processing Methods Comparison:")
        
        # Local processing simulation
        print("\nüè† Local Processing (MedGemma via Ollama):")
        print("   ‚úÖ Advantages:")
        print("     ‚Ä¢ Complete data privacy (HIPAA compliant)")
        print("     ‚Ä¢ No external API dependencies")
        print("     ‚Ä¢ Consistent processing times")
        print("     ‚Ä¢ No per-query costs")
        print("   ‚ö†Ô∏è  Requirements:")
        print("     ‚Ä¢ 16GB+ RAM for model loading")
        print("     ‚Ä¢ GPU acceleration recommended")
        print("     ‚Ä¢ Local model management")
        
        # API processing simulation
        print("\n‚òÅÔ∏è  API Processing (Google AI/Gemini):")
        print("   ‚úÖ Advantages:")
        print("     ‚Ä¢ Lower local resource requirements")
        print("     ‚Ä¢ Always latest model versions")
        print("     ‚Ä¢ Managed infrastructure")
        print("   ‚ö†Ô∏è  Considerations:")
        print("     ‚Ä¢ Medical data leaves premises")
        print("     ‚Ä¢ Internet dependency")
        print("     ‚Ä¢ Per-query API costs")
        print("     ‚Ä¢ Potential latency variations")
        
        # Recommend approach
        print("\nüí° Recommendation for Medical Use:")
        print("   üè• Hospital Production: Local processing (privacy + compliance)")
        print("   üß™ Development/Testing: API processing (convenience)")
        print("   üîÑ Hybrid: Local for sensitive data, API for research")
    
    async def run_complete_demo(self):
        """Run the complete medical AI demonstration."""
        print("üè• VIGIA MEDICAL AI COMPREHENSIVE DEMO")
        print("=" * 60)
        print("This demo showcases MedGemma integration for medical AI analysis")
        print("in pressure injury detection and clinical decision support.\n")
        
        # Initialize components
        print("üîß Initializing medical AI components...")
        if not await self.initialize():
            print("‚ùå Failed to initialize. Running limited demo...")
        
        # Run all demo sections
        try:
            await self.demo_basic_medical_analysis()
            await self.demo_medical_image_analysis()
            await self.demo_triage_workflow()
            await self.demo_prompt_engineering()
            await self.demo_local_vs_api_comparison()
            
        except Exception as e:
            logger.error(f"Demo execution error: {e}")
        
        # Summary
        print("\n" + "="*60)
        print("üìã DEMO SUMMARY")
        print("="*60)
        print("‚úÖ Completed medical AI workflow demonstrations:")
        print("   ‚Ä¢ Basic clinical analysis with MedGemma")
        print("   ‚Ä¢ Medical image analysis integration")
        print("   ‚Ä¢ Hospital triage workflow processing")
        print("   ‚Ä¢ Medical prompt engineering techniques")
        print("   ‚Ä¢ Local vs API processing comparison")
        print("\nüè• Ready for hospital production deployment!")
        print("üìö See docs/DEVELOPER_GUIDE.md for implementation details")


async def main():
    """Main demo execution."""
    try:
        # Environment check
        if not os.getenv('GOOGLE_API_KEY') and not os.path.exists('/usr/local/bin/ollama'):
            print("‚ö†Ô∏è  Neither Google API key nor Ollama installation detected.")
            print("Some features may be limited. See docs/SETUP_GUIDE.md for setup instructions.")
            print("-" * 60)
        
        # Run demo
        demo = MedicalAIDemo()
        await demo.run_complete_demo()
        
    except KeyboardInterrupt:
        print("\n\nüëã Demo interrupted by user")
    except Exception as e:
        print(f"\n‚ùå Demo failed: {e}")
        logger.exception("Demo execution failed")


if __name__ == "__main__":
    print("üß† Starting Vigia Medical AI Demo...")
    print("Press Ctrl+C to interrupt at any time\n")
    
    asyncio.run(main())